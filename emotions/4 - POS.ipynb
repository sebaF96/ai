{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e284e848",
   "metadata": {},
   "source": [
    "# Etiquetado POS\n",
    "\n",
    "El POS (part of speech) tagging o etiquetado morfológico es el proceso mediante el cual se clasifican las partes de un texto de acuerdo a su clasificación léxica.\n",
    "\n",
    "Cada palabra recibirá una clasificación léxica a partir de una colección de etiquetas codificadas de acuerdo a su significado en el idioma correspondiente. Para poder realizar un etiquetado POS el texto debe estar previamente tokenizado.\n",
    "\n",
    "NLKT ofrece una función llamada pos_tag. Esta función clasifica las palabras en ingés según un sistema de codificación pre-definido. Este etiquetador en particular está basado en machine learning y ha sido entrenado a partir de miles de ejemplos de oraciones pre-etiquetadas de manera manual. De esta manera puede estimar la clasificación léxica más probable de un término lo cuál no significa que esté libre de errores.\n",
    "\n",
    "Es posible obtener una lista completa de los códigos de etiquetado para NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6344d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordcloud')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c3cf1",
   "metadata": {},
   "source": [
    "Es posible obtener la descripción cada una categoría específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "374e5d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "[('texas', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"NNS\")\n",
    "tokenized_text = [\"texas\"]\n",
    "text_pos = nltk.pos_tag(tokenized_text)\n",
    "print(text_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2bc786",
   "metadata": {},
   "source": [
    "Ya que este etiquetador puede no ser suficiente bueno en algunos casos es posible mejorar la eficiencia del etiquetado sumando etiquetadores POS creados manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a3812",
   "metadata": {},
   "source": [
    "## Etiquetado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787a9e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Palace', 'of', 'Westminster', 'serves', 'as', 'the', 'meeting', 'place', 'for', 'both', 'the', 'House', 'of', 'Commons', 'and', 'the', 'House', 'of', 'Lords', ',', 'the', 'two', 'houses', 'of', 'the', 'Parliament', 'of', 'the', 'United', 'Kingdom', '.', 'Informally', 'known', 'as', 'the', 'Houses', 'of', 'Parliament', 'after', 'its', 'occupants', ',', 'the', 'Palace', 'lies', 'on', 'the', 'north', 'bank', 'of', 'the', 'River', 'Thames', 'in', 'the', 'City', 'of', 'Westminster', ',', 'in', 'central', 'London', ',', 'England', '.']\n",
      "[('The', 'DT'), ('Palace', 'NNP'), ('of', 'IN'), ('Westminster', 'NNP'), ('serves', 'NNS'), ('as', 'IN'), ('the', 'DT'), ('meeting', 'NN'), ('place', 'NN'), ('for', 'IN'), ('both', 'CC'), ('the', 'DT'), ('House', 'NNP'), ('of', 'IN'), ('Commons', 'NNPS'), ('and', 'CC'), ('the', 'DT'), ('House', 'NNP'), ('of', 'IN'), ('Lords', 'NNPS'), (',', ','), ('the', 'DT'), ('two', 'CD'), ('houses', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Parliament', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('Kingdom', 'NNP'), ('.', '.'), ('Informally', 'RB'), ('known', 'VBN'), ('as', 'IN'), ('the', 'DT'), ('Houses', 'NNP'), ('of', 'IN'), ('Parliament', 'NNP'), ('after', 'IN'), ('its', 'PRP$'), ('occupants', 'NNS'), (',', ','), ('the', 'DT'), ('Palace', 'NNP'), ('lies', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('north', 'JJ'), ('bank', 'NN'), ('of', 'IN'), ('the', 'DT'), ('River', 'NNP'), ('Thames', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('City', 'NNP'), ('of', 'IN'), ('Westminster', 'NNP'), (',', ','), ('in', 'IN'), ('central', 'JJ'), ('London', 'NNP'), (',', ','), ('England', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "example = \"The Palace of Westminster serves as the meeting place for both the House of Commons and the House of Lords, the two houses of the Parliament of the United Kingdom. Informally known as the Houses of Parliament after its occupants, the Palace lies on the north bank of the River Thames in the City of Westminster, in central London, England.\"\n",
    "# Tokenizar texto\n",
    "tokenized_text = nltk.word_tokenize(example)\n",
    "print(tokenized_text)\n",
    "# Etiquetar texto con pos_tag\n",
    "text_pos = nltk.pos_tag(tokenized_text)\n",
    "print(text_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29454864",
   "metadata": {},
   "source": [
    "# POS en español\n",
    "\n",
    "Si queremos hacer un etiquetado morfológico en otro idioma entonces es necesario encontrar un etiquetador ya entrenado para ese idioma o entrenar uno nosotros mismos. También es necesario saber cuales son las clasificaciones de palabras existentes para dicho idioma.\n",
    "\n",
    "En el siguiente <a href=\"https://colab.research.google.com/github/vitojph/kschool-nlp-18/blob/master/notebooks/pos-tagger-es.ipynb\">enlace se muestra un ejemplo práctico de etiquetado POS en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a13a298",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f1fbed6d312f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "# Cargar valores del archivo .env en las variables de entorno\n",
    "load_dotenv()\n",
    "# Cargar valor del token a variable\n",
    "bearer_token = os.environ.get(\"BEARER_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b8413",
   "metadata": {},
   "source": [
    "# Ejercicio\n",
    "\n",
    "- Obtener de la API una lista de Tweets que no sean retweet y que contengan el hashtag #GRAMMYs en inglés.\n",
    "- Realizar la tokenización\n",
    "- Realizar un etiquetado POS con la función pos_tag de NLTK\n",
    "- Obtener la lista y frecuencia de los sustantivos en singular y plural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f6e1a",
   "metadata": {},
   "source": [
    "# Ejercicio N°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'query': '#GRAMMYs  lang:en -is:retweet',\n",
    "    'tweet.fields':'created_at',\n",
    "    'max_results':100\n",
    "}\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "    \"User-Agent\":\"v2FullArchiveSearchPython\"\n",
    "}\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "print(response)\n",
    "# Generar excepción si la respuesta no es exitosa\n",
    "if response.status_code != 200:\n",
    "    raise Exception(response.status_code, response.text)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de11946",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(response.json()['data'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "tokenized_text = df['text'].apply(tt.tokenize)\n",
    "df[\"tokenized_text\"] = tokenized_text\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos POS\n",
    "\n",
    "data = []\n",
    "# Crear lista de palabras\n",
    "for x in tokenized_text:\n",
    "    for word in x:\n",
    "        data.append(word)\n",
    "\n",
    "# Etiquetar texto con pos_tag\n",
    "data_pos = nltk.pos_tag(data)\n",
    "print(data_pos)\n",
    "data_noun = []\n",
    "for k,v in data_pos:\n",
    "    if v in [\"NN\",\"NNS\"]:\n",
    "        data_noun.append(k)\n",
    "print(data_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d8c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener solo: NN - NNS\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Obtener frecuencia de cada término\n",
    "fdist = FreqDist(data_noun)\n",
    "# Convertir a dataframe\n",
    "df_fdist = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist.columns = ['Frequency']\n",
    "df_fdist.index.name = 'Term'\n",
    "df_fdist.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "df_fdist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b1b76",
   "metadata": {},
   "source": [
    "- Obtener la lista y frecuencia de los nombres propios en singular y plural\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71dcd8",
   "metadata": {},
   "source": [
    "# Ejercicio N°2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos POS\n",
    "\n",
    "data = []\n",
    "# Crear lista de palabras\n",
    "for x in tokenized_text:\n",
    "    for word in x:\n",
    "        data.append(word)\n",
    "\n",
    "# Etiquetar texto con pos_tag\n",
    "data_pos = nltk.pos_tag(data)\n",
    "print(data_pos)\n",
    "data_names = []\n",
    "for k,v in data_pos:\n",
    "    if v in [\"NNP\",\"NNPS\"]:\n",
    "        data_names.append(k)\n",
    "print(data_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67edefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener solo: NN - NNS\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Obtener frecuencia de cada término\n",
    "fdist = FreqDist(data_names)\n",
    "# Convertir a dataframe\n",
    "df_fdist = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist.columns = ['Frequency']\n",
    "df_fdist.index.name = 'Term'\n",
    "df_fdist.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "df_fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb056b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "191f4397",
   "metadata": {},
   "source": [
    "- Obtener la lista y frecuencia de los verbos en todos los tiempos verbales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1f3d8d",
   "metadata": {},
   "source": [
    "# Ejercicio N°3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1983c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos POS\n",
    "\n",
    "data = []\n",
    "# Crear lista de palabras\n",
    "for x in tokenized_text:\n",
    "    for word in x:\n",
    "        data.append(word)\n",
    "\n",
    "# Etiquetar texto con pos_tag\n",
    "data_pos = nltk.pos_tag(data)\n",
    "print(data_pos)\n",
    "data_verbs = []\n",
    "for k,v in data_pos:\n",
    "    if v in [\"VBZ\", \"VBP\", \"VBN\", \"VBG\", \"VBD\", \"VB\"]:\n",
    "        data_verbs.append(k)\n",
    "print(data_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e446b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Obtener frecuencia de cada término\n",
    "fdist = FreqDist(data_verbs)\n",
    "# Convertir a dataframe\n",
    "df_fdist = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist.columns = ['Frequency']\n",
    "df_fdist.index.name = 'Term'\n",
    "df_fdist.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "df_fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a5911",
   "metadata": {},
   "source": [
    "- Obtener la lista y frecuencia de todos los adjetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84afd7",
   "metadata": {},
   "source": [
    "# Ejercicio N°3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos POS\n",
    "\n",
    "data = []\n",
    "# Crear lista de palabras\n",
    "for x in tokenized_text:\n",
    "    for word in x:\n",
    "        data.append(word)\n",
    "\n",
    "# Etiquetar texto con pos_tag\n",
    "data_pos = nltk.pos_tag(data)\n",
    "print(data_pos)\n",
    "data_adjective = []\n",
    "for k,v in data_pos:\n",
    "    if v in [\"JJ\", \"JJR\", \"JJS\"]: # \"JJ\", \"JJR\", \"JJS\"\n",
    "        data_adjective.append(k)\n",
    "print(data_adjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc0b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Obtener frecuencia de cada término\n",
    "fdist = FreqDist(data_adjective)\n",
    "# Convertir a dataframe\n",
    "df_fdist = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist.columns = ['Frequency']\n",
    "df_fdist.index.name = 'Term'\n",
    "df_fdist.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "df_fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee458a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
